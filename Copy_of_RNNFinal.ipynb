{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Main**"
      ],
      "metadata": {
        "id": "ucUvftTVvNyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.formats.format import DataFrameFormatter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"BWP_rates.csv\")\n",
        "\n",
        "newdf = df\n",
        "newdf['date_float'] = pd.to_datetime(df['ds']).astype(int) / 10**9 / 86400\n",
        "newdf = df.drop(['ds'], axis=1)\n",
        "\n",
        "\n",
        "newdf = newdf.values.reshape(-1, 1)\n",
        "\n",
        "print(newdf)\n",
        "\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "newdf = scaler.fit_transform(newdf)\n",
        "\n",
        "\n",
        "# split into train and test sets\n",
        "train_data, test_data = traintest(newdf)\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(newdf, back=1):\n",
        "  dataX, dataY = [], []  \n",
        "  for i in range(len(newdf)-back-1):\n",
        "    a = newdf[i:(i+back), 0]\n",
        "    dataX.append(a)\n",
        "    dataY.append(newdf[i + back, 0])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "back = 1\n",
        "trainX, trainY = create_dataset(train_data, back)\n",
        "testX, testY = create_dataset(test_data, back)\n",
        "\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "\n",
        "\n",
        "# create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(4, input_shape=(1, back)))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
        "\n",
        "\n",
        "# make predictions\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "# invert predictions\n",
        "trainPredict = scaler.inverse_transform(trainPredict)\n",
        "trainY = scaler.inverse_transform([trainY])\n",
        "testPredict = scaler.inverse_transform(testPredict)\n",
        "testY = scaler.inverse_transform([testY])\n",
        "# calculate root mean squared error\n",
        "trainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n",
        "\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(newdf)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[back:len(trainPredict)+back, :] = trainPredict\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(newdf)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(trainPredict)+(back*2)+1:len(newdf)-1, :] = testPredict\n",
        "# plot baseline and predictions\n",
        "plt.plot(scaler.inverse_transform(newdf))\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yJzcA5_mu1RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to run the training and testing data**"
      ],
      "metadata": {
        "id": "x5ybqMTzvBIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def traintest(newdf):\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train_data, test_data = train_test_split(newdf, test_size=0.2, shuffle=False, random_state=9)\n",
        "\n",
        "    # Print the number of rows in each set\n",
        "    print(f'Training set contains {len(train_data)} rows.')\n",
        "    print(f'Testing set contains {len(test_data)} rows.')\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "print(traintest(newdf))"
      ],
      "metadata": {
        "id": "a7iW1jOdu_ro"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}